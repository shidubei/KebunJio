{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm  # For progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python script to generate csv\n",
    "\n",
    "dataset_path = \"C:/Users/azril/Downloads/Dataset_v1/Train\"\n",
    "\n",
    "op_csv = \"C:/Users/azril/Downloads/Dataset_v1/csv/dataset_v1.csv\"\n",
    "\n",
    "with open(op_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['S/N', 'Plant', 'Healthy/NotHealthy','Filepath'])\n",
    "\n",
    "    serial_number = 1  # Initialize serial number\n",
    "\n",
    "    for plant_folder in os.listdir(dataset_path):\n",
    "        plant_path = os.path.join(dataset_path, plant_folder)\n",
    "\n",
    "        if os.path.isdir(plant_path):\n",
    "            for status in ['Healthy', 'NotHealthy']:\n",
    "                status_path = os.path.join(plant_path, status)\n",
    "\n",
    "                if os.path.isdir(status_path):\n",
    "                    for image_file in os.listdir(status_path): \n",
    "                        image_path = os.path.join(status_path, image_file)\n",
    "                        if os.path.isfile(image_path):\n",
    "                            writer.writerow([serial_number, plant_folder, status, image_path])\n",
    "                            serial_number += 1  # Increment serial number\n",
    "\n",
    "print(f'csv file saved to {op_csv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Transforms: Compose(\n",
      "    Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)\n",
      "    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.2, 0.2))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "Validation Transforms: Compose(\n",
      "    Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for training (Data Augmentation)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images to a fixed size\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.RandomRotation(15),  # Rotate the image randomly by up to 15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Random color jitter\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image with ImageNet values\n",
    "])\n",
    "\n",
    "# Define transformations for validation/testing (No Augmentation, only resizing and normalization)\n",
    "valid_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images to a fixed size\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image with ImageNet values\n",
    "])\n",
    "\n",
    "# Print out the transformations to verify\n",
    "print(\"Training Transforms:\", train_transforms)\n",
    "print(\"Validation Transforms:\", valid_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantHealthCheckDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)  # Load the CSV\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx, 2]  # Image path from CSV\n",
    "        label = self.data.iloc[idx, 1]  # Healthy/NotHealthy label\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = Image.open(img_path)  # Open the image\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformations\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PlantHealthCheckDataset(\n",
    "    csv_file='path_to_train_csv', \n",
    "    root_dir='path_to_train_images', \n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "valid_dataset = PlantHealthCheckDataset(\n",
    "    csv_file='path_to_valid_csv', \n",
    "    root_dir='path_to_valid_images', \n",
    "    transform=valid_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PlantHealthCheck CNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantHealthCheckCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PlantHealthCheckCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers with ReLU activation and Max Pooling\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)  # Adjust input size based on image size after convolutions\n",
    "        self.fc2 = nn.Linear(512, 2)  # Output 2 classes: Healthy, NotHealthy\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply convolution, activation (ReLU), and pooling for each layer\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)  # Max Pooling\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)  # Max Pooling\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)  # Max Pooling\n",
    "        \n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten (batch_size, 128 * 16 * 16)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Output layer (2 classes: Healthy, NotHealthy)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, num_epochs=10, lr=0.001):\n",
    "    # Define optimizer (Adam optimizer)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Define loss function (CrossEntropyLoss for classification)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Track training and validation losses\n",
    "    train_loss_history = []\n",
    "    valid_loss_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "            images, labels = images.to(device), labels.to(device)  # Move to GPU if available\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss and accuracy\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "        \n",
    "        # Average loss for this epoch\n",
    "        train_loss /= total_train\n",
    "        train_loss_history.append(train_loss)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        valid_loss = 0.0\n",
    "        correct_valid = 0\n",
    "        total_valid = 0\n",
    "        \n",
    "        with torch.no_grad():  # No need to compute gradients for validation\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                valid_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_valid += (predicted == labels).sum().item()\n",
    "                total_valid += labels.size(0)\n",
    "        \n",
    "        # Average validation loss for this epoch\n",
    "        valid_loss /= total_valid\n",
    "        valid_loss_history.append(valid_loss)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}\")\n",
    "        print(f\"Train Accuracy: {100 * correct_train / total_train:.2f}%, Validation Accuracy: {100 * correct_valid / total_valid:.2f}%\")\n",
    "    \n",
    "    return model, train_loss_history, valid_loss_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
